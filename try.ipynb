{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook rag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')\n",
    "# from transformers import RagRetriever\n",
    "\n",
    "# retriever = RagRetriever.from_pretrained(\n",
    "#     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO du kan lage datasettet til selv pÃ¥ den mÃ¥ten\n",
    "#TODO da fÃ¥r du testet faiss index ogsÃ¥ som er interessant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset code from fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List, Optional\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from datasets import Features, Sequence, Value, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \") -> List[str]:\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict):\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "\n",
    "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast):\n",
    "    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n",
    "    input_ids = ctx_tokenizer(\n",
    "        documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n",
    "    return {\"embeddings\": embeddings.detach().cpu().numpy()}\n",
    "\n",
    "\n",
    "def main(\n",
    "    rag_example_args: \"RagExampleArguments\",\n",
    "    processing_args: \"ProcessingArguments\",\n",
    "    index_hnsw_args: \"IndexHnswArguments\",\n",
    "):\n",
    "    ######################################\n",
    "    logger.info(\"Step 1 - Create the dataset\")\n",
    "    ######################################\n",
    "\n",
    "    # The dataset needed for RAG must have three columns:\n",
    "    # - title (string): title of the document\n",
    "    # - text (string): text of a passage of the document\n",
    "    # - embeddings (array of dimension d): DPR representation of the passage\n",
    "\n",
    "    # Let's say you have documents in tab-separated csv files with columns \"title\" and \"text\"\n",
    "    assert os.path.isfile(rag_example_args.csv_path), \"Please provide a valid path to a csv file\"\n",
    "\n",
    "    # You can load a Dataset object this way\n",
    "    dataset = load_dataset(\n",
    "        \"csv\", data_files=[rag_example_args.csv_path], split=\"train\", delimiter=\"\\t\", column_names=[\"title\", \"text\"]\n",
    "    )\n",
    "\n",
    "    # More info about loading csv files in the documentation: https://huggingface.co/docs/datasets/loading_datasets?highlight=csv#csv-files\n",
    "\n",
    "    # Then split the documents into passages of 100 words\n",
    "    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n",
    "\n",
    "    # And compute the embeddings\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n",
    "    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n",
    "    new_features = Features(\n",
    "        {\"text\": Value(\"string\"), \"title\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))}\n",
    "    )  # optional, save as float32 instead of float64 to save space\n",
    "    dataset = dataset.map(\n",
    "        partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=processing_args.batch_size,\n",
    "        features=new_features,\n",
    "    )\n",
    "\n",
    "    # And finally save your dataset\n",
    "    passages_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset\")\n",
    "    dataset.save_to_disk(passages_path)\n",
    "    # from datasets import load_from_disk\n",
    "    # dataset = load_from_disk(passages_path)  # to reload the dataset\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 2 - Index the dataset\")\n",
    "    ######################################\n",
    "\n",
    "    # Let's use the Faiss implementation of HNSW for fast approximate nearest neighbor search\n",
    "    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n",
    "    dataset.add_faiss_index(\"embeddings\", custom_index=index)\n",
    "\n",
    "    # And save the index\n",
    "    index_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset_hnsw_index.faiss\")\n",
    "    dataset.get_index(\"embeddings\").save(index_path)\n",
    "    # dataset.load_faiss_index(\"embeddings\", index_path)  # to reload the index\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 3 - Load RAG\")\n",
    "    ######################################\n",
    "\n",
    "    # Easy way to load the model\n",
    "    retriever = RagRetriever.from_pretrained(\n",
    "        rag_example_args.rag_model_name, index_name=\"custom\", indexed_dataset=dataset\n",
    "    )\n",
    "    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n",
    "    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n",
    "\n",
    "    # For distributed fine-tuning you'll need to provide the paths instead, as the dataset and the index are loaded separately.\n",
    "    # retriever = RagRetriever.from_pretrained(rag_model_name, index_name=\"custom\", passages_path=passages_path, index_path=index_path)\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 4 - Have fun\")\n",
    "    ######################################\n",
    "\n",
    "    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n",
    "    input_ids = tokenizer.question_encoder(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    generated = model.generate(input_ids)\n",
    "    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "    logger.info(\"Q: \" + question)\n",
    "    logger.info(\"A: \" + generated_string)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RagExampleArguments:\n",
    "    csv_path: str = field(\n",
    "        default=str(Path(__file__).parent / \"test_data\" / \"my_knowledge_dataset.csv\"),\n",
    "        metadata={\"help\": \"Path to a tab-separated csv file with columns 'title' and 'text'\"},\n",
    "    )\n",
    "    question: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Question that is passed as input to RAG. Default is 'What does Moses' rod turn into ?'.\"},\n",
    "    )\n",
    "    rag_model_name: str = field(\n",
    "        default=\"facebook/rag-sequence-nq\",\n",
    "        metadata={\"help\": \"The RAG model to use. Either 'facebook/rag-sequence-nq' or 'facebook/rag-token-nq'\"},\n",
    "    )\n",
    "    dpr_ctx_encoder_model_name: str = field(\n",
    "        default=\"facebook/dpr-ctx_encoder-multiset-base\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The DPR context encoder model to use. Either 'facebook/dpr-ctx_encoder-single-nq-base' or\"\n",
    "                \" 'facebook/dpr-ctx_encoder-multiset-base'\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    output_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a directory where the dataset passages and the index will be saved\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingArguments:\n",
    "    num_proc: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The number of processes to use to split the documents into passages. Default is single process.\"\n",
    "        },\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=16,\n",
    "        metadata={\n",
    "            \"help\": \"The batch size to use when computing the passages embeddings using the DPR context encoder.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IndexHnswArguments:\n",
    "    d: int = field(\n",
    "        default=768,\n",
    "        metadata={\"help\": \"The dimension of the embeddings to pass to the HNSW Faiss index.\"},\n",
    "    )\n",
    "    m: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The number of bi-directional links created for every new element during the HNSW index construction.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.WARNING)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    parser = HfArgumentParser((RagExampleArguments, ProcessingArguments, IndexHnswArguments))\n",
    "    rag_example_args, processing_args, index_hnsw_args = parser.parse_args_into_dataclasses()\n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        rag_example_args.output_dir = rag_example_args.output_dir or tmp_dir\n",
    "        main(rag_example_args, processing_args, index_hnsw_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/v1.6.2/faiss_and_ea.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1000.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss \n",
    "\n",
    "data = {\n",
    "    'title': ['Title1', 'Title2'],\n",
    "    'text': ['Text content 1', 'Text content 2'],\n",
    "    'embeddings': [np.random.rand(128).astype('float32'), np.random.rand(128).astype('float32')]  # Sample embeddings\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a Dataset from the DataFrame\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Assuming 'embeddings' column is your embeddings and they are in float32 format\n",
    "# You might need to adjust the dimension size (128 here) based on your actual embeddings\n",
    "dimension = 128\n",
    "index = faiss.IndexFlatL2(dimension)  # Use IndexFlatL2 for L2 distance, or choose another appropriate index type\n",
    "\n",
    "# Add the FAISS index to the dataset - the embeddings need to be of type List[float]\n",
    "dataset = dataset.add_faiss_index(column='embeddings', custom_index=index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Rag with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO remove later\n",
    "# from transformers import RagTokenizer, RagRetriever, RagConfig, RagTokenForGeneration\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# # Create a configuration object (make sure to specify the correct configs)\n",
    "# config = RagConfig.from_pretrained(\n",
    "#     \"facebook/rag-token-nq\",\n",
    "#     question_encoder=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "#     generator=\"facebook/bart-large\",\n",
    "# )\n",
    "\n",
    "# # Initialize RagTokenForGeneration with the config\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# from transformers import RagRetriever\n",
    "# retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration \n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummindexed_datasety_dataset=True) \n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", indexed_dataset=dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eivin\\.cache\\huggingface\\hub\\models--facebook--rag-sequence-nq. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\tokenization_rag.py:87: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_dict = tokenizer.prepare_seq2seq_batch(\"how many countries are in europe\", return_tensors=\"pt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2129, 2116, 3032, 2024, 1999, 2885,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\modeling_rag.py:980\u001b[0m, in \u001b[0;36mRagSequenceForGeneration.generate\u001b[1;34m(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     question_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 980\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# set to correct device\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m context_input_ids\u001b[38;5;241m.\u001b[39mto(input_ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:621\u001b[0m, in \u001b[0;36mRagRetriever.__call__\u001b[1;34m(self, question_input_ids, question_hidden_states, prefix, n_docs, return_tensors)\u001b[0m\n\u001b[0;32m    619\u001b[0m n_docs \u001b[38;5;241m=\u001b[39m n_docs \u001b[38;5;28;01mif\u001b[39;00m n_docs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_docs\n\u001b[0;32m    620\u001b[0m prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mprefix\n\u001b[1;32m--> 621\u001b[0m retrieved_doc_embeds, doc_ids, docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m input_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(question_input_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    624\u001b[0m context_input_ids, context_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_docs(\n\u001b[0;32m    625\u001b[0m     docs, input_strings, prefix, n_docs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors\n\u001b[0;32m    626\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:570\u001b[0m, in \u001b[0;36mRagRetriever.retrieve\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[0;32m    552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    Retrieves documents for specified `question_hidden_states`.\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m        - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     doc_ids, retrieved_doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_doc_embeds, doc_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_doc_dicts(doc_ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:540\u001b[0m, in \u001b[0;36mRagRetriever._main_retrieve\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_hidden_states \u001b[38;5;129;01min\u001b[39;00m question_hidden_states_batched:\n\u001b[0;32m    539\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 540\u001b[0m     ids, vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex search time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_hidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m     )\n\u001b[0;32m    544\u001b[0m     ids_batched\u001b[38;5;241m.\u001b[39mextend(ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:230\u001b[0m, in \u001b[0;36mHFIndexBase.get_top_docs\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m--> 230\u001b[0m     _, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m    232\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\datasets\\search.py:727\u001b[0m, in \u001b[0;36mIndexableMixin.search_batch\u001b[1;34m(self, index_name, queries, k, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the nearest examples indices in the dataset to the query.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m        - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_index_is_initialized(index_name)\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes[index_name]\u001b[38;5;241m.\u001b[39msearch_batch(queries, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\datasets\\search.py:378\u001b[0m, in \u001b[0;36mFaissIndex.search_batch\u001b[1;34m(self, queries, k, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m queries\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n\u001b[0;32m    377\u001b[0m     queries \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(queries, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaiss_index\u001b[38;5;241m.\u001b[39msearch(queries, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchedSearchResults(scores, indices\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\faiss\\class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[0;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generated = model.generate(input_ids=input_dict[\"input_ids\"])  #TODO error here, assertion error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# should give 54 => google says either 44 or 51\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\modeling_rag.py:980\u001b[0m, in \u001b[0;36mRagSequenceForGeneration.generate\u001b[1;34m(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     question_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 980\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# set to correct device\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m context_input_ids\u001b[38;5;241m.\u001b[39mto(input_ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:621\u001b[0m, in \u001b[0;36mRagRetriever.__call__\u001b[1;34m(self, question_input_ids, question_hidden_states, prefix, n_docs, return_tensors)\u001b[0m\n\u001b[0;32m    619\u001b[0m n_docs \u001b[38;5;241m=\u001b[39m n_docs \u001b[38;5;28;01mif\u001b[39;00m n_docs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_docs\n\u001b[0;32m    620\u001b[0m prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mprefix\n\u001b[1;32m--> 621\u001b[0m retrieved_doc_embeds, doc_ids, docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m input_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_encoder_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(question_input_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    624\u001b[0m context_input_ids, context_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_docs(\n\u001b[0;32m    625\u001b[0m     docs, input_strings, prefix, n_docs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors\n\u001b[0;32m    626\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:570\u001b[0m, in \u001b[0;36mRagRetriever.retrieve\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[0;32m    552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    Retrieves documents for specified `question_hidden_states`.\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m        - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     doc_ids, retrieved_doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_doc_embeds, doc_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_doc_dicts(doc_ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:540\u001b[0m, in \u001b[0;36mRagRetriever._main_retrieve\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_hidden_states \u001b[38;5;129;01min\u001b[39;00m question_hidden_states_batched:\n\u001b[0;32m    539\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 540\u001b[0m     ids, vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex search time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_hidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m     )\n\u001b[0;32m    544\u001b[0m     ids_batched\u001b[38;5;241m.\u001b[39mextend(ids)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:230\u001b[0m, in \u001b[0;36mHFIndexBase.get_top_docs\u001b[1;34m(self, question_hidden_states, n_docs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, question_hidden_states: np\u001b[38;5;241m.\u001b[39mndarray, n_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m--> 230\u001b[0m     _, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m    232\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\datasets\\search.py:727\u001b[0m, in \u001b[0;36mIndexableMixin.search_batch\u001b[1;34m(self, index_name, queries, k, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the nearest examples indices in the dataset to the query.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m        - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_index_is_initialized(index_name)\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes[index_name]\u001b[38;5;241m.\u001b[39msearch_batch(queries, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\datasets\\search.py:378\u001b[0m, in \u001b[0;36mFaissIndex.search_batch\u001b[1;34m(self, queries, k, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m queries\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n\u001b[0;32m    377\u001b[0m     queries \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(queries, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaiss_index\u001b[38;5;241m.\u001b[39msearch(queries, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchedSearchResults(scores, indices\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\eivin\\Documents\\Programming\\RAG_testing\\venv\\lib\\site-packages\\faiss\\class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[0;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "\n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) \n",
    "\n",
    "# should give 54 => google says either 44 or 51\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
